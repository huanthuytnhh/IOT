import os
from dotenv import load_dotenv
from collections import defaultdict, Counter
import pyaudio
import wave
from pydub import AudioSegment
import speech_recognition as sr
import numpy as np
import time
# from flask import Flask, request, jsonify # Commented out Flask imports
import threading
# import requests # Commented out requests as it's used for web communication
import ast

# Import c√°c l·ªõp t√πy ch·ªânh
from devices.servo import ServoController
from devices.motor import MotorController
from devices.stepper import StepperController
from devices.led import Led
from devices.dht11 import DHTSensor
from devices.touch import TouchSensor
import speaker_recognition.neural_net as neural_net
import speaker_recognition.inference as inference
from db.db_helper import query_members_files, query_permissions, connect_db
from utils import convert_sample_rate, extract_action_and_device, speak_text, extend_audio
import logging
import json
# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
# Load bi·∫øn m√¥i tr∆∞·ªùng t·ª´ .env
load_dotenv()
import tempfile
# C·∫•u h√¨nh bi·∫øn m√¥i tr∆∞·ªùng
SPEAKER_RECOGNITION_MODEL_PATH = os.getenv("SPEAKER_RECOGNITION_MODEL_PATH")
DB_PATH = os.getenv("DB_PATH")
N_TIMES_DUPLICATE = int(os.getenv("N_TIMES_DUPLICATE"))
N_TAKEN_AUDIO = int(os.getenv("N_TAKEN_AUDIO"))
K_NEAREST_NEIGHBOURS = int(os.getenv("K_NEAREST_NEIGHBOURS"))
FORMAT = eval(os.getenv("FORMAT"))
CHANNELS = int(os.getenv("CHANNELS"))
RATE = int(os.getenv("RATE"))
CHUNK = int(os.getenv("CHUNK"))
RAW_RECORDING_PATH = os.getenv("RAW_RECORDING_PATH")
RESAMPLED_RATE = int(os.getenv("RESAMPLED_RATE"))
WAVE_OUTPUT_RAW_FILENAME = os.getenv("WAVE_OUTPUT_RAW_FILENAME")
WAVE_OUTPUT_RESAMPLED_FILENAME = os.getenv("WAVE_OUTPUT_RESAMPLED_FILENAME")
MAC_ADDRESS = os.getenv("MAC_ADDRESS")

# Kh·ªüi t·∫°o m√¥ h√¨nh nh·∫≠n di·ªán gi·ªçng n√≥i
SPEAKER_RECOGNITION_MODEL = neural_net.get_speaker_encoder(SPEAKER_RECOGNITION_MODEL_PATH)

# K·∫øt n·ªëi database
CONN = connect_db(DB_PATH)

# Kh·ªüi t·∫°o c√°c thi·∫øt b·ªã t√πy ch·ªânh
motor = MotorController( enable_pin=14,in1_pin=15,in2_pin=18 )
stepper = StepperController(21, 20, 16, 12)
servo_parent = ServoController(7)
# servo_children = ServoController(8) # Commented out as it's commented in your original
led_living = Led(4)
led_kitchen = Led(17)
led_children = Led(10)
led_parent = Led(11)
led_garage = Led(5)
dht = DHTSensor(13, 19, 26)
touch_sensor = TouchSensor(22)

# T·ª´ ƒëi·ªÉn tr·∫°ng th√°i thi·∫øt b·ªã
status_data = {
    "Garage Led": 0,
    "Garage Door": 0,
    "Living Led": 0,
    "Kitchen Led": 0,
    "Parent Led": 0,
    "Children Led": 0,
    "Temperature": 0,
    "Humidity": 0,
}
# --- H√†m h·ªó tr·ª£ ---
def get_unique_filename(base_filename):
    """T·∫°o t√™n file duy nh·∫•t b·∫±ng c√°ch th√™m timestamp."""
    base, ext = os.path.splitext(base_filename)
    return f"{base}_{int(time.time())}{ext}"

def convert_sample_rate(input_filename, output_filename, target_sample_rate):
    """Chuy·ªÉn ƒë·ªïi t·∫ßn s·ªë l·∫•y m·∫´u c·ªßa file WAV."""
    # print(f"ƒêang chuy·ªÉn ƒë·ªïi sample rate c·ªßa '{input_filename}' sang {target_sample_rate} Hz...")
    try:
        sound = AudioSegment.from_file(input_filename)
        sound = sound.set_frame_rate(target_sample_rate)
        sound.export(output_filename, format="wav")
        # print(f"ƒê√£ l∆∞u file chuy·ªÉn ƒë·ªïi: '{output_filename}'")
    except Exception as e:
        print(f"L·ªói khi chuy·ªÉn ƒë·ªïi sample rate: {e}")
        raise

def extend_audio(audio_segment, times=N_TIMES_DUPLICATE):
    """Nh√¢n b·∫£n ƒëo·∫°n √¢m thanh ƒë·ªÉ k√©o d√†i th·ªùi l∆∞·ª£ng."""
    if times > 1:
        print(f"Nh√¢n b·∫£n √¢m thanh {times} l·∫ßn ƒë·ªÉ k√©o d√†i th·ªùi l∆∞·ª£ng...")
    return audio_segment * times

# Redefinition from original code, keeping it as is per request
def convert_sample_rate(input_filename, output_filename, target_sample_rate):
    sound = AudioSegment.from_file(input_filename)
    sound = sound.set_frame_rate(target_sample_rate)
    sound.export(output_filename, format="wav")

# Redefinition from original code, keeping it as is per request
def extend_audio(audio, times=5):
    return audio * times

def get_embedding_from_audiosegment(audio, encoder):
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmpfile:
        audio.export(tmpfile.name, format="wav")
        embedding = inference.get_embedding(tmpfile.name, encoder)
    os.remove(tmpfile.name)
    return embedding

# H√†m chu·∫©n b·ªã embedding cho m·∫´u gi·ªçng n√≥i
def prepare_base_embedding(file_path):
    try:
        audio = AudioSegment.from_wav(file_path)
        extended = extend_audio(audio, times=N_TIMES_DUPLICATE)
        return get_embedding_from_audiosegment(extended, SPEAKER_RECOGNITION_MODEL)
    except Exception as e:
        print(f"L·ªói khi x·ª≠ l√Ω file √¢m thanh {file_path}: {e}")
        return None

# Load embedding c·ªßa c√°c ng∆∞·ªùi d√πng
print("üîÑ ƒêang load m·∫´u gi·ªçng n√≥i...")
user_embeddings = {}
# try:
#     user_embeddings = {
#         "Tr√≠": prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples/Tri-Merge_Audio.wav"),
#         "Ph√°t": prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples/Phat-Merge_Audio.wav"),
#         "L√™ Ng·ªçc Thanh": prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples/Thanh-Merge_Audio.wav"),
#         "L∆∞u Duy Quang": prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples/Quang-Merge_Audio.wav"),
#         "Ng√¥ Nguy·ªÖn T·∫•n Qu√¢n": prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples/Quan-Merge_Audio.wav"),
#         "Phan Thanh Sum": prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples/Sum-Merge_Audio.wav"),
#         "ƒê·∫°t": prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples/Dat-Merge_Audio.wav"),
#     }
#     # Lo·∫°i b·ªè c√°c embedding None (n·∫øu c√≥ l·ªói)
#     user_embeddings = {k: v for k, v in user_embeddings.items() if v is not None}
#     if not user_embeddings:
#         raise Exception("Kh√¥ng load ƒë∆∞·ª£c embedding n√†o!")
#     print("‚úÖ ƒê√£ load xong t·∫•t c·∫£ c√°c m·∫´u gi·ªçng n√≥i!")
# except Exception as e:
#     print(f"L·ªói khi load m·∫´u gi·ªçng n√≥i: {e}")
#     exit(1)

# Kh·ªüi t·∫°o ·ª©ng d·ª•ng Flask
# app = Flask(__name__) # Commented out Flask app initialization

# H√†m ƒëi·ªÅu khi·ªÉn thi·∫øt b·ªã
def control_device(device, action):
    try:
        if device == "c·ª≠a ph√≤ng kh√°ch":
            motor.open_door_close_door(time_to_wait=3, open_duration=2, close_duration=2, speed=0.35)
            print(f"ƒê√£ {action} c·ª≠a ph√≤ng kh√°ch")
        elif device == "c·ª≠a nh√† xe":
            if action == "m·ªü":
                stepper.rotate("forward", 5)
                status_data["Garage Door"] = 1
            elif action == "ƒë√≥ng":
                stepper.rotate("backward", 5)
                status_data["Garage Door"] = 0
        # elif device == "c·ª≠a ph√≤ng ng·ªß con c√°i": # Commented out as it's commented in your original
        #     servo_children.open_door_close_door(0, 6)
        elif device == "c·ª≠a ph√≤ng ng·ªß ba m·∫π":
            servo_parent.open_door_close_door(180, 6)
        elif device == "ƒë√®n ph√≤ng kh√°ch":
            if action == "b·∫≠t":
                led_living.on()
                status_data["Living Led"] = 1
            elif action == "t·∫Øt":
                led_living.off()
                status_data["Living Led"] = 0
        elif device == "ƒë√®n ph√≤ng b·∫øp":
            if action == "b·∫≠t":
                led_kitchen.on()
                status_data["Kitchen Led"] = 1
            elif action == "t·∫Øt":
                led_kitchen.off()
                status_data["Kitchen Led"] = 0
        elif device == "ƒë√®n ph√≤ng ng·ªß ba m·∫π":
            if action == "b·∫≠t":
                led_parent.on()
                status_data["Parent Led"] = 1
            elif action == "t·∫Øt":
                led_parent.off()
                status_data["Parent Led"] = 0
        elif device == "ƒë√®n ph√≤ng ng·ªß con c√°i":
            if action == "b·∫≠t":
                led_children.on()
                status_data["Children Led"] = 1
            elif action == "t·∫Øt":
                led_children.off()
                status_data["Children Led"] = 0
        elif device == "ƒë√®n nh√† xe":
            if action == "b·∫≠t":
                led_garage.on()
                status_data["Garage Led"] = 1
            elif action == "t·∫Øt":
                led_garage.off()
                status_data["Garage Led"] = 0
        elif device == "c·∫£m bi·∫øn":
            if action == "xem":
                humidity, temperature = dht.read_dht11()
                status_data["Humidity"] = humidity
                status_data["Temperature"] = temperature
                speak_text(f"Nhi·ªát ƒë·ªô hi·ªán t·∫°i l√† {temperature} ƒë·ªô C v√† ƒë·ªô ·∫©m l√† {humidity} %")
        # G·ª≠i tr·∫°ng th√°i c·∫≠p nh·∫≠t ƒë·∫øn ESP32 - Commented out as part of web communication
        # try:
        #     response = requests.post("http://192.168.1.199:10000/message", json=status_data)
        #     if response.status_code == 200:
        #         print("ƒê√£ g·ª≠i tr·∫°ng th√°i ƒë·∫øn ESP32")
        #     else:
        #         print(f"L·ªói khi g·ª≠i tr·∫°ng th√°i: {response.status_code}")
        # except Exception as e:
        #     print(f"L·ªói khi g·ª≠i tr·∫°ng th√°i ƒë·∫øn ESP32: {e}")
    except Exception as e:
        print(f"L·ªói khi ƒëi·ªÅu khi·ªÉn thi·∫øt b·ªã: {e}")

# ƒêi·ªÉm cu·ªëi Flask ƒë·ªÉ nh·∫≠n l·ªánh t·ª´ ESP32 - Commented out Flask endpoint
# @app.route('/command', methods=['POST'])
# def command():
#     try:
#         data = request.get_json()
#         device = data.get('device')
#         action = data.get('action')
#         if device and action:
#             control_device(device, action)
#             return jsonify({"status": "success", "current_state": status_data}), 200
#         else:
#             return jsonify({"status": "error", "message": "L·ªánh kh√¥ng h·ª£p l·ªá"}), 400
#     except Exception as e:
#         print(f"L·ªói trong endpoint Flask: {e}")
#         return jsonify({"status": "error", "message": "L·ªói x·ª≠ l√Ω y√™u c·∫ßu"}), 500

# Ch·∫°y Flask trong m·ªôt lu·ªìng ri√™ng - Commented out Flask thread
# def run_flask():
#     app.run(host='0.0.0.0', port=5000, threaded=True)

# flask_thread = threading.Thread(target=run_flask)
# flask_thread.daemon = True
# flask_thread.start()

# H√†m ghi √¢m v√† nh·∫≠n di·ªán gi·ªçng n√≥i
import json
import numpy as np
from collections import defaultdict, Counter
import pyaudio
import wave
from pydub import AudioSegment
import speech_recognition as sr
import time
from scipy.spatial.distance import cosine
import logging

logger = logging.getLogger(__name__)

def record_audio():
    audio = pyaudio.PyAudio()  # Initialize PyAudio
    try:
        stream = audio.open(format=FORMAT, channels=CHANNELS,
                            rate=RATE, input=True,
                            frames_per_buffer=CHUNK)

        print("Recording...")
        frames = []
        start_time = time.time()
        # Record while sensor is touched or for at least 2 seconds
        while touch_sensor.is_touched() or (time.time() - start_time < 2):
            data = stream.read(CHUNK, exception_on_overflow=False)
            frames.append(data)
            elapsed_time = time.time() - start_time
            print(f"Recording time: {elapsed_time:.2f} seconds", end="\r")

        stream.stop_stream()
        stream.close()
        print("\nRecording finished.")
    except Exception as e:
        print(f"Error during audio recording: {e}")
        return
    finally:
        audio.terminate()

    # Save raw audio
    try:
        with wave.open(WAVE_OUTPUT_RAW_FILENAME, 'wb') as wf:
            wf.setnchannels(CHANNELS)
            wf.setsampwidth(audio.get_sample_size(FORMAT))
            wf.setframerate(RATE)
            wf.writeframes(b''.join(frames))
        print(f"Raw audio saved to {WAVE_OUTPUT_RAW_FILENAME}")
    except Exception as e:
        print(f"Error saving audio file: {e}")
        return

    # Process audio (resample and duplicate)
    try:
        convert_sample_rate(WAVE_OUTPUT_RAW_FILENAME, WAVE_OUTPUT_RESAMPLED_FILENAME, RESAMPLED_RATE)
        sound = AudioSegment.from_file(WAVE_OUTPUT_RESAMPLED_FILENAME, format="wav")
        duplicated_sound = sound * N_TIMES_DUPLICATE
        duplicated_sound.export(WAVE_OUTPUT_RESAMPLED_FILENAME, format="wav")
        print(f"Resampled and duplicated audio saved to {WAVE_OUTPUT_RESAMPLED_FILENAME}")
    except Exception as e:
        print(f"Error processing audio file: {e}")
        return

    # Speaker recognition
    try:
        # Query member files from the database
        member_files = query_members_files(CONN)
        logger.info(f"Retrieved {len(member_files)} member files from database.")

        # Process database embeddings
        user_embeddings = defaultdict(list)
        expected_length = 128  # Match input embedding dimension
        for i, member_file in enumerate(member_files):
            features = member_file['features']
            logger.info(f"Member File {i+1}:")
            logger.info(f"  Member ID: {member_file['member_id']}")
            logger.info(f"  Member Name: {member_file['member_name']}")
            logger.info(f"  File Path: {member_file['file_path']}")

            if features is None:
                logger.warning(f"  Warning: Features are None for {member_file['member_name']} (file: {member_file['file_path']})")
                continue
            elif isinstance(features, str):
                try:
                    parsed_features = json.loads(features)
                    embedding = np.array(parsed_features)
                    logger.info(f"  Parsed Feature Shape: {embedding.shape}, Type: {type(embedding)}")
                    if len(embedding.shape) == 2:  # If 2D array (e.g., (60, 128))
                        embedding = np.mean(embedding, axis=0)  # Average over first dimension
                    elif len(embedding.shape) != 1:
                        logger.warning(f"  Warning: Unexpected embedding shape {embedding.shape} for {member_file['member_name']}")
                        continue
                    logger.info(f"  Aggregated Feature Length: {len(embedding)}")
                except json.JSONDecodeError as e:
                    logger.error(f"  Error: Failed to parse features string for {member_file['member_name']}: {e}")
                    logger.info(f"  Raw Features: {features}")
                    continue
            else:
                embedding = np.array(features).flatten()
                logger.info(f"  Feature Length: {len(embedding)}")
                logger.info(f"  Feature Shape: {embedding.shape}")

            if len(embedding) != expected_length:
                logger.warning(f"  Skipping {member_file['member_name']} (file: {member_file['file_path']}): Embedding length {len(embedding)} does not match expected {expected_length}")
                continue
            user_embeddings[member_file['member_name']].append((embedding, member_file['file_path']))
            logger.info("---------------------------------\n")

        if not user_embeddings:
            logger.error("No valid embeddings for comparison.")
            return

        # Query permissions
        permissions = query_permissions(CONN)
        check_permission = defaultdict(lambda: defaultdict(bool))
        for permission in permissions:
            check_permission[permission.member_name][permission.appliance_name] = True

        # Get audio embedding for the input WAV file
        audio_file_embedding = inference.get_embedding(WAVE_OUTPUT_RESAMPLED_FILENAME, SPEAKER_RECOGNITION_MODEL)
        if audio_file_embedding is None:
            logger.error("Failed to generate embedding for input WAV file.")
            return
        audio_file_embedding = np.array(audio_file_embedding).flatten()
        logger.info(f"Input WAV embedding shape: {audio_file_embedding.shape}, sample: {audio_file_embedding[:5]}")

        # Validate input embedding length
        if len(audio_file_embedding) != expected_length:
            logger.warning(f"Input WAV embedding length {len(audio_file_embedding)} does not match database embedding length {expected_length}.")
            if len(audio_file_embedding) > expected_length:
                audio_file_embedding = audio_file_embedding[:expected_length]
                logger.info(f"Truncated input embedding to length {expected_length}")
            else:
                audio_file_embedding = np.pad(audio_file_embedding, (0, expected_length - len(audio_file_embedding)), mode='constant')
                logger.info(f"Padded input embedding to length {expected_length}")

        # Compute cosine similarities
        all_distances = []
        mean_distances = {}
        for speaker in user_embeddings:
            distances = []
            for emb, file_path in user_embeddings[speaker]:
                try:
                    similarity = 1 - cosine(audio_file_embedding, emb)
                    if similarity is not None:
                        all_distances.append((similarity, speaker, file_path))
                        distances.append(similarity)
                    else:
                        logger.warning(f"Cosine similarity returned None for {speaker} (file: {file_path})")
                except Exception as e:
                    logger.warning(f"Error computing cosine similarity for {speaker} (file: {file_path}): {e}")
                    continue
            mean_distances[speaker] = np.mean(distances) if distances else float('-inf')

        if not all_distances:
            logger.error("No valid cosine similarities computed.")
            return

        # KNN prediction
        K_NEAREST_NEIGHBOURS = 5  # Adjust as needed
        sorted_distances = sorted(all_distances, key=lambda x: x[0], reverse=True)
        knn_predictions = [speaker for _, speaker, _ in sorted_distances[:K_NEAREST_NEIGHBOURS]]
        predicted_speaker_knn = Counter(knn_predictions).most_common(1)[0][0]

        # Mean distance prediction
        predicted_speaker_mean = max(mean_distances, key=mean_distances.get)
        max_mean_distance = mean_distances[predicted_speaker_mean]

        # Print results
        logger.info("\n--- Mean Cosine Similarity ---")
        for speaker, distance in mean_distances.items():
            logger.info(f"  - {speaker}: {distance:.4f}")
        logger.info(f"\n--- K-Nearest Neighbors Prediction (K={K_NEAREST_NEIGHBOURS}) ---")
        logger.info(f"Top {K_NEAREST_NEIGHBOURS} nearest neighbors: {knn_predictions}")
        logger.info(f"\033[94mPredicted Speaker (KNN): {predicted_speaker_knn}\033[0m")
        logger.info(f"\n--- Mean Distance Prediction ---")
        logger.info(f"Predicted Speaker (Mean): {predicted_speaker_mean} (similarity: {max_mean_distance:.4f})")

        # Speech recognition
        recognizer = sr.Recognizer()
        with sr.AudioFile(WAVE_OUTPUT_RAW_FILENAME) as source:
            audio_data = recognizer.record(source)
            try:
                content = recognizer.recognize_google(audio_data, language="vi-VN").lower()
                print("Recognized text: ", content)
            except sr.UnknownValueError:
                print("Could not recognize text from audio.")
                return
            except sr.RequestError as e:
                print(f"Error in speech recognition request: {e}")
                return

        action, device = extract_action_and_device(content)
        print(f"Action: {action} Device: {device}")

        predicted_speaker = predicted_speaker_knn  # Use KNN prediction
        if action is None or device is None:
            speak_text("Thi·∫øt b·ªã ho·∫∑c h√†nh ƒë·ªông kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c")
            print(f"\033[92mThi·∫øt b·ªã ho·∫∑c h√†nh ƒë·ªông kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c\033[0m")
        elif check_permission[predicted_speaker][device]:
            speak_text(f"Xin ch√†o {predicted_speaker}. B·∫°n c√≥ quy·ªÅn {action} {device}")
            print(f"\033[92m{predicted_speaker} c√≥ quy·ªÅn {action} {device}\033[0m")
            control_device(device, action)
        else:
            speak_text(f"Xin ch√†o {predicted_speaker}. B·∫°n kh√¥ng c√≥ quy·ªÅn {action} {device}")
            print(f"\033[91m{predicted_speaker} kh√¥ng c√≥ quy·ªÅn {action} {device}\033[0m")

    except Exception as e:
        logger.error(f"Error in speaker recognition: {e}")
        return
# V√≤ng l·∫∑p ch√≠nh
try:
    print("Ready...")
    # Th√™m bi·∫øn tr·∫°ng th√°i ƒë·ªÉ tr√°nh ghi √¢m li√™n t·ª•c khi sensor v·∫´n ch·∫°m
    is_recording_active = False 
    
    while True:
        # ƒê·ªçc d·ªØ li·ªáu DHT11 v√† c·∫≠p nh·∫≠t tr·∫°ng th√°i (n·∫øu c·∫ßn g·ª≠i ƒëi, nh∆∞ng hi·ªán t·∫°i ƒë√£ comment)
        try:
            humidity, temperature = dht.read_dht11()
            # print(f"Humidity: {humidity}, Temperature: {temperature}")
            if humidity is not None and temperature is not None:
                status_data["Humidity"] = humidity
                status_data["Temperature"] = temperature
        except Exception as e:
            print(f"L·ªói khi ƒë·ªçc c·∫£m bi·∫øn DHT11: {e}")

        # G·ª≠i tr·∫°ng th√°i c·∫≠p nh·∫≠t ƒë·∫øn ESP32 - Commented out
        # try:
        #     response = requests.post("http://192.168.1.4:10000/message", json=status_data)
        #     if response.status_code == 200:
        #         print("ƒê√£ g·ª≠i tr·∫°ng th√°i ƒë·∫øn ESP32")
        #     else:
        #         print(f"L·ªói khi g·ª≠i tr·∫°ng th√°i: {response.status_code}")
        # except Exception as e:
        #     print(f"L·ªói khi g·ª≠i tr·∫°ng th√°i ƒë·∫øn ESP32: {e}")

        # Logic ƒëi·ªÅu khi·ªÉn b·∫±ng c·∫£m bi·∫øn ch·∫°m
        if touch_sensor.is_touched() and not is_recording_active:
            print("C·∫£m bi·∫øn ch·∫°m ƒë∆∞·ª£c nh·∫•n!")
            is_recording_active = True # ƒê·∫∑t c·ªù l√† ƒëang ghi √¢m
            record_audio()
        elif not touch_sensor.is_touched() and is_recording_active:
            # Reset c·ªù khi c·∫£m bi·∫øn kh√¥ng c√≤n ch·∫°m (ho·∫∑c sau khi ghi √¢m ƒë√£ ho√†n th√†nh)
            is_recording_active = False
            print("C·∫£m bi·∫øn ch·∫°m ƒë√£ nh·∫£.")
            
        time.sleep(0.1)  # NgƒÉn s·ª≠ d·ª•ng CPU qu√° m·ª©c
except KeyboardInterrupt:
    print("D·ª´ng ch∆∞∆°ng tr√¨nh...")
except Exception as e:
    print(f"L·ªói kh√¥ng mong mu·ªën trong v√≤ng l·∫∑p ch√≠nh: {e}")
finally:
    print("D·ªçn d·∫πp t√†i nguy√™n...")
    # ƒê·∫£m b·∫£o c√°c thi·∫øt b·ªã GPIO ƒë∆∞·ª£c ƒë√≥ng ƒë√∫ng c√°ch
    servo_parent.servo.close()
    led_living.close()
    led_kitchen.close()
    led_children.close()
    led_parent.close()
    led_garage.close()
    motor.pwm.close()
    motor.in1.close()
    motor.in2.close()
    stepper.step_pins.close()
    stepper.step_sequence.close()
    stepper.steps_per_revolution.close()
    stepper.step_sequence.close()
    # N·∫øu c√≥ sensor kh√°c c·∫ßn ƒë√≥ng, th√™m v√†o ƒë√¢y
    touch_sensor.sensor.close()