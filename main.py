import os
from dotenv import load_dotenv
from collections import defaultdict, Counter
import pyaudio
import wave
from pydub import AudioSegment
import speech_recognition as sr
import numpy as np
import time
# from flask import Flask, request, jsonify # Commented out Flask imports
import threading
# import requests # Commented out requests as it's used for web communication

# Import c√°c l·ªõp t√πy ch·ªânh
from devices.servo import ServoController
from devices.motor import MotorController
from devices.stepper import StepperController
from devices.led import Led
from devices.dht11 import DHTSensor
from devices.touch import TouchSensor
import speaker_recognition.neural_net as neural_net
import speaker_recognition.inference as inference
from db.db_helper import query_members, query_permissions, connect_db
from utils import convert_sample_rate, extract_action_and_device, speak_text, extend_audio

# Load bi·∫øn m√¥i tr∆∞·ªùng t·ª´ .env
load_dotenv()
import tempfile
# C·∫•u h√¨nh bi·∫øn m√¥i tr∆∞·ªùng
SPEAKER_RECOGNITION_MODEL_PATH = os.getenv("SPEAKER_RECOGNITION_MODEL_PATH")
DB_PATH = os.getenv("DB_PATH")
N_TIMES_DUPLICATE = int(os.getenv("N_TIMES_DUPLICATE"))
N_TAKEN_AUDIO = int(os.getenv("N_TAKEN_AUDIO"))
K_NEAREST_NEIGHBOURS = int(os.getenv("K_NEAREST_NEIGHBOURS"))
FORMAT = eval(os.getenv("FORMAT"))
CHANNELS = int(os.getenv("CHANNELS"))
RATE = int(os.getenv("RATE"))
CHUNK = int(os.getenv("CHUNK"))
RAW_RECORDING_PATH = os.getenv("RAW_RECORDING_PATH")
RESAMPLED_RATE = int(os.getenv("RESAMPLED_RATE"))
WAVE_OUTPUT_RAW_FILENAME = os.getenv("WAVE_OUTPUT_RAW_FILENAME")
WAVE_OUTPUT_RESAMPLED_FILENAME = os.getenv("WAVE_OUTPUT_RESAMPLED_FILENAME")
MAC_ADDRESS = os.getenv("MAC_ADDRESS")

# Kh·ªüi t·∫°o m√¥ h√¨nh nh·∫≠n di·ªán gi·ªçng n√≥i
SPEAKER_RECOGNITION_MODEL = neural_net.get_speaker_encoder(SPEAKER_RECOGNITION_MODEL_PATH)

# K·∫øt n·ªëi database
CONN = connect_db(DB_PATH)

# Kh·ªüi t·∫°o c√°c thi·∫øt b·ªã t√πy ch·ªânh
motor = MotorController(14,15,18)
stepper = StepperController(21, 20, 16, 12)
servo_parent = ServoController(7)
# servo_children = ServoController(8) # Commented out as it's commented in your original
led_living = Led(4)
led_kitchen = Led(17)
led_children = Led(10)
led_parent = Led(11)
led_garage = Led(5)
dht = DHTSensor(13, 19, 26)
touch_sensor = TouchSensor(22)

# T·ª´ ƒëi·ªÉn tr·∫°ng th√°i thi·∫øt b·ªã
status_data = {
    "Garage Led": 0,
    "Garage Door": 0,
    "Living Led": 0,
    "Kitchen Led": 0,
    "Parent Led": 0,
    "Children Led": 0,
    "Temperature": 0,
    "Humidity": 0,
}
# --- H√†m h·ªó tr·ª£ ---
def get_unique_filename(base_filename):
    """T·∫°o t√™n file duy nh·∫•t b·∫±ng c√°ch th√™m timestamp."""
    base, ext = os.path.splitext(base_filename)
    return f"{base}_{int(time.time())}{ext}"

def convert_sample_rate(input_filename, output_filename, target_sample_rate):
    """Chuy·ªÉn ƒë·ªïi t·∫ßn s·ªë l·∫•y m·∫´u c·ªßa file WAV."""
    # print(f"ƒêang chuy·ªÉn ƒë·ªïi sample rate c·ªßa '{input_filename}' sang {target_sample_rate} Hz...")
    try:
        sound = AudioSegment.from_file(input_filename)
        sound = sound.set_frame_rate(target_sample_rate)
        sound.export(output_filename, format="wav")
        # print(f"ƒê√£ l∆∞u file chuy·ªÉn ƒë·ªïi: '{output_filename}'")
    except Exception as e:
        print(f"L·ªói khi chuy·ªÉn ƒë·ªïi sample rate: {e}")
        raise

def extend_audio(audio_segment, times=N_TIMES_DUPLICATE):
    """Nh√¢n b·∫£n ƒëo·∫°n √¢m thanh ƒë·ªÉ k√©o d√†i th·ªùi l∆∞·ª£ng."""
    if times > 1:
        print(f"Nh√¢n b·∫£n √¢m thanh {times} l·∫ßn ƒë·ªÉ k√©o d√†i th·ªùi l∆∞·ª£ng...")
    return audio_segment * times

# Redefinition from original code, keeping it as is per request
def convert_sample_rate(input_filename, output_filename, target_sample_rate):
    sound = AudioSegment.from_file(input_filename)
    sound = sound.set_frame_rate(target_sample_rate)
    sound.export(output_filename, format="wav")

# Redefinition from original code, keeping it as is per request
def extend_audio(audio, times=5):
    return audio * times

def get_embedding_from_audiosegment(audio, encoder):
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmpfile:
        audio.export(tmpfile.name, format="wav")
        embedding = inference.get_embedding(tmpfile.name, encoder)
    os.remove(tmpfile.name)
    return embedding

# H√†m chu·∫©n b·ªã embedding cho m·∫´u gi·ªçng n√≥i
def prepare_base_embedding(file_path):
    try:
        audio = AudioSegment.from_wav(file_path)
        extended = extend_audio(audio, times=N_TIMES_DUPLICATE)
        return get_embedding_from_audiosegment(extended, SPEAKER_RECOGNITION_MODEL)
    except Exception as e:
        print(f"L·ªói khi x·ª≠ l√Ω file √¢m thanh {file_path}: {e}")
        return None

# Load embedding c·ªßa c√°c ng∆∞·ªùi d√πng
print("üîÑ ƒêang load m·∫´u gi·ªçng n√≥i...")
user_embeddings = {}
try:
    user_embeddings = {
        "Tr√≠": prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples/Tri-Merge_Audio.wav"),
        "Ph√°t": prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples/Phat-Merge_Audio.wav"),
        "Thanh": prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples/Thanh-Merge_Audio.wav"),
        "Quang": prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples/Quang-Merge_Audio.wav"),
        "Qu√¢n": prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples/Quan-Merge_Audio.wav"),
        "Sum": prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples/Sum-Merge_Audio.wav"),
        "ƒê·∫°t": prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples/Dat-Merge_Audio.wav"),
    }
    # Lo·∫°i b·ªè c√°c embedding None (n·∫øu c√≥ l·ªói)
    user_embeddings = {k: v for k, v in user_embeddings.items() if v is not None}
    if not user_embeddings:
        raise Exception("Kh√¥ng load ƒë∆∞·ª£c embedding n√†o!")
    print("‚úÖ ƒê√£ load xong t·∫•t c·∫£ c√°c m·∫´u gi·ªçng n√≥i!")
except Exception as e:
    print(f"L·ªói khi load m·∫´u gi·ªçng n√≥i: {e}")
    exit(1)

# Kh·ªüi t·∫°o ·ª©ng d·ª•ng Flask
# app = Flask(__name__) # Commented out Flask app initialization

# H√†m ƒëi·ªÅu khi·ªÉn thi·∫øt b·ªã
def control_device(device, action):
    try:
        if device == "c·ª≠a nh√† xe":
            if action == "m·ªü":
                stepper.rotate("forward", 5)
                status_data["Garage Door"] = 1
            elif action == "ƒë√≥ng":
                stepper.rotate("backward", 5)
                status_data["Garage Door"] = 0
        # elif device == "c·ª≠a ph√≤ng ng·ªß con c√°i": # Commented out as it's commented in your original
        #     servo_children.open_door_close_door(0, 6)
        elif device == "c·ª≠a ph√≤ng ng·ªß ba m·∫π":
            servo_parent.open_door_close_door(0, 6)
        elif device == "ƒë√®n ph√≤ng kh√°ch":
            if action == "b·∫≠t":
                led_living.on()
                status_data["Living Led"] = 1
            elif action == "t·∫Øt":
                led_living.off()
                status_data["Living Led"] = 0
        elif device == "ƒë√®n ph√≤ng b·∫øp":
            if action == "b·∫≠t":
                led_kitchen.on()
                status_data["Kitchen Led"] = 1
            elif action == "t·∫Øt":
                led_kitchen.off()
                status_data["Kitchen Led"] = 0
        elif device == "ƒë√®n ph√≤ng ng·ªß ba m·∫π":
            if action == "b·∫≠t":
                led_parent.on()
                status_data["Parent Led"] = 1
            elif action == "t·∫Øt":
                led_parent.off()
                status_data["Parent Led"] = 0
        elif device == "ƒë√®n ph√≤ng ng·ªß con c√°i":
            if action == "b·∫≠t":
                led_children.on()
                status_data["Children Led"] = 1
            elif action == "t·∫Øt":
                led_children.off()
                status_data["Children Led"] = 0
        elif device == "ƒë√®n nh√† xe":
            if action == "b·∫≠t":
                led_garage.on()
                status_data["Garage Led"] = 1
            elif action == "t·∫Øt":
                led_garage.off()
                status_data["Garage Led"] = 0
        elif device == "c·∫£m bi·∫øn":
            if action == "xem":
                humidity, temperature = dht.read_dht11()
                status_data["Humidity"] = humidity
                status_data["Temperature"] = temperature
                speak_text(f"Nhi·ªát ƒë·ªô hi·ªán t·∫°i l√† {temperature} ƒë·ªô C v√† ƒë·ªô ·∫©m l√† {humidity} %")
        # G·ª≠i tr·∫°ng th√°i c·∫≠p nh·∫≠t ƒë·∫øn ESP32 - Commented out as part of web communication
        # try:
        #     response = requests.post("http://192.168.1.4:10000/message", json=status_data)
        #     if response.status_code == 200:
        #         print("ƒê√£ g·ª≠i tr·∫°ng th√°i ƒë·∫øn ESP32")
        #     else:
        #         print(f"L·ªói khi g·ª≠i tr·∫°ng th√°i: {response.status_code}")
        # except Exception as e:
        #     print(f"L·ªói khi g·ª≠i tr·∫°ng th√°i ƒë·∫øn ESP32: {e}")
    except Exception as e:
        print(f"L·ªói khi ƒëi·ªÅu khi·ªÉn thi·∫øt b·ªã: {e}")

# ƒêi·ªÉm cu·ªëi Flask ƒë·ªÉ nh·∫≠n l·ªánh t·ª´ ESP32 - Commented out Flask endpoint
# @app.route('/command', methods=['POST'])
# def command():
#     try:
#         data = request.get_json()
#         device = data.get('device')
#         action = data.get('action')
#         if device and action:
#             control_device(device, action)
#             return jsonify({"status": "success", "current_state": status_data}), 200
#         else:
#             return jsonify({"status": "error", "message": "L·ªánh kh√¥ng h·ª£p l·ªá"}), 400
#     except Exception as e:
#         print(f"L·ªói trong endpoint Flask: {e}")
#         return jsonify({"status": "error", "message": "L·ªói x·ª≠ l√Ω y√™u c·∫ßu"}), 500

# Ch·∫°y Flask trong m·ªôt lu·ªìng ri√™ng - Commented out Flask thread
# def run_flask():
#     app.run(host='0.0.0.0', port=5000, threaded=True)

# flask_thread = threading.Thread(target=run_flask)
# flask_thread.daemon = True
# flask_thread.start()

# H√†m ghi √¢m v√† nh·∫≠n di·ªán gi·ªçng n√≥i
def record_audio():
    audio = pyaudio.PyAudio()  # Kh·ªüi t·∫°o PyAudio trong h√†m
    try:
        stream = audio.open(format=FORMAT, channels=CHANNELS,
                             rate=RATE, input=True,
                             frames_per_buffer=CHUNK)

        print("Recording...")

        frames = []
        start_time = time.time()
        # Ghi √¢m cho ƒë·∫øn khi c·∫£m bi·∫øn nh·∫£ ra (n·∫øu kh√¥ng c√≥ h√†nh ƒë·ªông k√©o d√†i audio)
        # ho·∫∑c ghi √¢m m·ªôt kho·∫£ng th·ªùi gian t·ªëi thi·ªÉu ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªß d·ªØ li·ªáu
        while touch_sensor.is_touched() or (time.time() - start_time < 2): # Ghi √≠t nh·∫•t 2 gi√¢y ho·∫∑c cho ƒë·∫øn khi nh·∫£ sensor
            data = stream.read(CHUNK)
            frames.append(data)
            elapsed_time = time.time() - start_time
            print(f"Recording time: {elapsed_time:.2f} seconds", end="\r")

        stream.stop_stream()
        stream.close()
        print("\nRecording finished.") # Newline after recording time updates
    except Exception as e:
        print(f"Error during audio recording: {e}")
        return
    finally:
        audio.terminate()  # ƒê·∫£m b·∫£o ƒë√≥ng PyAudio

    try:
        with wave.open(WAVE_OUTPUT_RAW_FILENAME, 'wb') as wf:
            wf.setnchannels(CHANNELS)
            wf.setsampwidth(audio.get_sample_size(FORMAT))
            wf.setframerate(RATE)
            wf.writeframes(b''.join(frames))
        print(f"Raw audio saved to {WAVE_OUTPUT_RAW_FILENAME}")
    except Exception as e:
        print(f"L·ªói khi ghi file √¢m thanh: {e}")
        return

    try:
        # Re-using the utility functions as per your original structure
        # convert_sample_rate is defined twice in original, using the latter one
        convert_sample_rate(WAVE_OUTPUT_RAW_FILENAME, WAVE_OUTPUT_RESAMPLED_FILENAME, RESAMPLED_RATE)
        sound = AudioSegment.from_file(WAVE_OUTPUT_RESAMPLED_FILENAME, format="wav")
        duplicated_sound = sound * N_TIMES_DUPLICATE
        duplicated_sound.export(WAVE_OUTPUT_RESAMPLED_FILENAME, format="wav")
        print(f"Resampled and duplicated audio saved to {WAVE_OUTPUT_RESAMPLED_FILENAME}")
    except Exception as e:
        print(f"Error processing audio file: {e}")
        return

    try:
        members = query_members(CONN)
        permissions = query_permissions(CONN)
        check_permission = defaultdict(lambda: defaultdict(lambda: False))
        for permission in permissions:
            check_permission[permission.member_name][permission.appliance_name] = True

        audio_file_embedding = inference.get_embedding(WAVE_OUTPUT_RESAMPLED_FILENAME, SPEAKER_RECOGNITION_MODEL)
        speaker_cosine_similarity = {}

        for member_name, embedding in user_embeddings.items():
            if embedding is not None:
                similarity = inference.compute_cosine_similarity(embedding, audio_file_embedding)
                speaker_cosine_similarity[member_name] = similarity
                print(f"\033[94m {member_name}: {similarity}\033[0m")

        if not speaker_cosine_similarity:
            print("Kh√¥ng c√≥ embedding h·ª£p l·ªá ƒë·ªÉ so s√°nh.")
            return

        predicted_speaker = max(speaker_cosine_similarity, key=speaker_cosine_similarity.get)
        print(f"\033[92mNg∆∞·ªùi ƒë∆∞·ª£c d·ª± ƒëo√°n: {predicted_speaker}\033[0m")
    except Exception as e:
        print(f"Error in speaker recognition: {e}")
        return

    try:
        recognizer = sr.Recognizer()
        with sr.AudioFile(WAVE_OUTPUT_RAW_FILENAME) as source:
            audio_data = recognizer.record(source)
            try:
                content = recognizer.recognize_google(audio_data, language="vi-VN").lower()
                print("VƒÉn b·∫£n ƒë∆∞·ª£c nh·∫≠n d·∫°ng: ", content)
            except sr.UnknownValueError:
                print("Kh√¥ng th·ªÉ nh·∫≠n d·∫°ng vƒÉn b·∫£n t·ª´ √¢m thanh.")
                return
            except sr.RequestError as e:
                print("L·ªói trong qu√° tr√¨nh g·ª≠i y√™u c·∫ßu: ", e)
                return
    except Exception as e:
        print(f"Error in speech recognition: {e}")
        return

    action, device = extract_action_and_device(content)
    print(f"Action: {action} Device: {device}")

    if action is None or device is None:
        speak_text("Thi·∫øt b·ªã ho·∫∑c h√†nh ƒë·ªông kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c")
        print(f"\033[92mThi·∫øt b·ªã ho·∫∑c h√†nh ƒë·ªông kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c\033[0m")
    elif check_permission[predicted_speaker][device]:
        speak_text(f"Xin ch√†o {predicted_speaker}. B·∫°n c√≥ quy·ªÅn {action} {device}")
        print(f"\033[92m{predicted_speaker} c√≥ quy·ªÅn {action} {device}\033[0m")
        control_device(device, action)
    else:
        speak_text(f"Xin ch√†o {predicted_speaker}. B·∫°n kh√¥ng c√≥ quy·ªÅn {action} {device}")
        print(f"\033[91m{predicted_speaker} kh√¥ng c√≥ quy·ªÅn {action} {device}\033[0m")

# V√≤ng l·∫∑p ch√≠nh
try:
    print("Ready...")
    # Th√™m bi·∫øn tr·∫°ng th√°i ƒë·ªÉ tr√°nh ghi √¢m li√™n t·ª•c khi sensor v·∫´n ch·∫°m
    is_recording_active = False 
    
    while True:
        # ƒê·ªçc d·ªØ li·ªáu DHT11 v√† c·∫≠p nh·∫≠t tr·∫°ng th√°i (n·∫øu c·∫ßn g·ª≠i ƒëi, nh∆∞ng hi·ªán t·∫°i ƒë√£ comment)
        try:
            humidity, temperature = dht.read_dht11()
            if humidity is not None and temperature is not None:
                status_data["Humidity"] = humidity
                status_data["Temperature"] = temperature
        except Exception as e:
            print(f"L·ªói khi ƒë·ªçc c·∫£m bi·∫øn DHT11: {e}")

        # G·ª≠i tr·∫°ng th√°i c·∫≠p nh·∫≠t ƒë·∫øn ESP32 - Commented out
        # try:
        #     response = requests.post("http://192.168.1.4:10000/message", json=status_data)
        #     if response.status_code == 200:
        #         print("ƒê√£ g·ª≠i tr·∫°ng th√°i ƒë·∫øn ESP32")
        #     else:
        #         print(f"L·ªói khi g·ª≠i tr·∫°ng th√°i: {response.status_code}")
        # except Exception as e:
        #     print(f"L·ªói khi g·ª≠i tr·∫°ng th√°i ƒë·∫øn ESP32: {e}")

        # Logic ƒëi·ªÅu khi·ªÉn b·∫±ng c·∫£m bi·∫øn ch·∫°m
        if touch_sensor.is_touched() and not is_recording_active:
            print("C·∫£m bi·∫øn ch·∫°m ƒë∆∞·ª£c nh·∫•n!")
            is_recording_active = True # ƒê·∫∑t c·ªù l√† ƒëang ghi √¢m
            record_audio()
        elif not touch_sensor.is_touched() and is_recording_active:
            # Reset c·ªù khi c·∫£m bi·∫øn kh√¥ng c√≤n ch·∫°m (ho·∫∑c sau khi ghi √¢m ƒë√£ ho√†n th√†nh)
            is_recording_active = False
            print("C·∫£m bi·∫øn ch·∫°m ƒë√£ nh·∫£.")
            
        time.sleep(0.1)  # NgƒÉn s·ª≠ d·ª•ng CPU qu√° m·ª©c
except KeyboardInterrupt:
    print("D·ª´ng ch∆∞∆°ng tr√¨nh...")
except Exception as e:
    print(f"L·ªói kh√¥ng mong mu·ªën trong v√≤ng l·∫∑p ch√≠nh: {e}")
finally:
    print("D·ªçn d·∫πp t√†i nguy√™n...")
    # ƒê·∫£m b·∫£o c√°c thi·∫øt b·ªã GPIO ƒë∆∞·ª£c ƒë√≥ng ƒë√∫ng c√°ch
    servo_parent.servo.close()
    led_living.close()
    led_kitchen.close()
    led_children.close()
    led_parent.close()
    led_garage.close()
    motor.pwm.close()
    motor.in1.close()
    motor.in2.close()
    stepper.step_pins.close()
    stepper.step_sequence.close()
    stepper.steps_per_revolution.close()
    stepper.step_sequence.close()
    # N·∫øu c√≥ sensor kh√°c c·∫ßn ƒë√≥ng, th√™m v√†o ƒë√¢y
    touch_sensor.sensor.close()