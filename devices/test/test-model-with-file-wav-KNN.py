import os
import time
import wave
import tempfile
import numpy as np
import torch
import pyaudio
from pydub import AudioSegment
from gpiozero import Button
import speaker_recognition.inference as inference
import speaker_recognition.neural_net as neural_net
from collections import Counter

# --- ƒê·ªãnh nghƒ©a c√°c h·∫±ng s·ªë ---
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000  # Match model input
CHUNK = 256
RECORD_SECONDS = 3  # 3 seconds for robust embeddings
RAW_RECORDING_FILENAME = "test-new-model-mic/recorded_audio.wav"
RESAMPLED_PROCESSING_FILENAME = "test-new-model-mic/processed_audio.wav"
N_TIMES_DUPLICATE = 1  # Dynamic extension in function
K_NEAREST_NEIGHBOURS = 5  # KNN parameter from second script

# --- C·∫•u h√¨nh ƒë·∫ßu v√†o ---
USE_MICROPHONE_INPUT = True
STATIC_WAV_FILE_PATH = "09_06/IOT/devices/test/test-new-model-mic/temp_audio_recording_resampled_raw_1749555457.wav"
os.makedirs("test-new-model-mic", exist_ok=True)

# C·∫£m bi·∫øn ch·∫°m GPIO
touch_sensor = Button(22)

# --- H√†m h·ªó tr·ª£ ---
def get_unique_filename(base_filename):
    """T·∫°o t√™n file duy nh·∫•t b·∫±ng c√°ch th√™m timestamp."""
    base, ext = os.path.splitext(base_filename)
    return f"{base}_{int(time.time())}{ext}"

def convert_sample_rate(input_filename, output_filename, target_sample_rate):
    """Chuy·ªÉn ƒë·ªïi t·∫ßn s·ªë l·∫•y m·∫´u c·ªßa file WAV."""
    try:
        sound = AudioSegment.from_file(input_filename)
        sound = sound.set_frame_rate(target_sample_rate)
        sound.export(output_filename, format="wav")
    except Exception as e:
        print(f"L·ªói khi chuy·ªÉn ƒë·ªïi sample rate: {e}")
        raise

def extend_audio(audio_segment, times=N_TIMES_DUPLICATE):
    """Nh√¢n b·∫£n ƒëo·∫°n √¢m thanh ƒë·ªÉ k√©o d√†i th·ªùi l∆∞·ª£ng."""
    if times > 1:
        print(f"Nh√¢n b·∫£n √¢m thanh {times} l·∫ßn ƒë·ªÉ k√©o d√†i th·ªùi l∆∞·ª£ng...")
    return audio_segment * times

def get_embedding_from_audiosegment(audio_segment, encoder_model):
    """L·∫•y embedding t·ª´ m·ªôt ƒë·ªëi t∆∞·ª£ng AudioSegment."""
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmpfile:
        audio_segment.export(tmpfile.name, format="wav")
        tmp_filename = tmpfile.name
    try:
        embedding = inference.get_embedding(tmp_filename, encoder_model)
    except Exception as e:
        print(f"L·ªói khi t√≠nh embedding cho '{tmp_filename}': {e}")
        raise
    finally:
        os.remove(tmp_filename)
    return embedding

def record_audio(filename=RAW_RECORDING_FILENAME, record_seconds=RECORD_SECONDS):
    """Ghi √¢m t·ª´ micro v√† l∆∞u v√†o file WAV."""
    print(f"üé§ ƒêang ghi √¢m trong {record_seconds} gi√¢y...")
    audio = pyaudio.PyAudio()
    try:
        stream = audio.open(format=FORMAT,
                            channels=CHANNELS,
                            rate=RATE,
                            input=True,
                            frames_per_buffer=CHUNK)
    except Exception as e:
        print(f"‚ùå L·ªói m·ªü stream √¢m thanh: {e}")
        audio.terminate()
        return None

    frames = []
    try:
        for _ in range(0, int(RATE / CHUNK * record_seconds)):
            data = stream.read(CHUNK, exception_on_overflow=False)
            frames.append(data)
    except Exception as e:
        print(f"‚ùå L·ªói khi ghi √¢m: {e}")
        return None
    finally:
        stream.stop_stream()
        stream.close()
        audio.terminate()

    full_filename = get_unique_filename(filename)
    try:
        wf = wave.open(full_filename, 'wb')
        wf.setnchannels(CHANNELS)
        wf.setsampwidth(audio.get_sample_size(FORMAT))
        wf.setframerate(RATE)
        wf.writeframes(b''.join(frames))
        wf.close()
    except Exception as e:
        print(f"‚ùå L·ªói l∆∞u file √¢m thanh: {e}")
        return None

    audio_segment = AudioSegment.from_wav(full_filename)
    normalized_audio = audio_segment.normalize()
    normalized_audio.export(full_filename, format="wav")
    print(f"ƒê√£ chu·∫©n h√≥a √¢m l∆∞·ª£ng v√† l∆∞u b·∫£n ghi √¢m t·∫°i: '{full_filename}'")
    
    if len(audio_segment) < 3000:  # Less than 3 seconds
        print("‚ö†Ô∏è C·∫£nh b√°o: B·∫£n ghi √¢m n√™n d√†i √≠t nh·∫•t 3 gi√¢y ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªô ch√≠nh x√°c.")
    return full_filename

def prepare_base_embedding(folder_path, encoder_model):
    """T√≠nh to√°n nhi·ªÅu embedding c∆° s·ªü t·ª´ th∆∞ m·ª•c ch·ª©a c√°c file WAV c·ªßa m·ªôt ng∆∞·ªùi d√πng."""
    if not os.path.exists(folder_path):
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c '{folder_path}'.")
        return None

    audio_files = [f for f in os.listdir(folder_path) if f.endswith(".wav")]
    if not audio_files:
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file WAV trong '{folder_path}'.")
        return None

    embeddings = []
    for audio_file in audio_files[:10]:  # Limit to 10 files per speaker for efficiency
        file_path = os.path.join(folder_path, audio_file)
        try:
            audio = AudioSegment.from_wav(file_path)
            if len(audio) < 3000:
                print(f"‚ö†Ô∏è File '{file_path}' ng·∫Øn h∆°n 3 gi√¢y ({len(audio)} ms). N√™n d√πng file 3‚Äì5 gi√¢y.")
            audio = audio.normalize()
            resampled_path = get_unique_filename("test-new-model-mic/temp_resampled_base.wav")
            convert_sample_rate(file_path, resampled_path, 16000)
            audio = AudioSegment.from_wav(resampled_path)
            audio = audio.normalize()
            extended = extend_audio(audio)
            embedding = get_embedding_from_audiosegment(extended, encoder_model)
            embeddings.append((embedding, os.path.basename(folder_path)))  # Store embedding and speaker label
            os.remove(resampled_path)
        except Exception as e:
            print(f"L·ªói x·ª≠ l√Ω file '{file_path}': {e}")
            continue
    if not embeddings:
        print(f"‚ùå Kh√¥ng t·∫°o ƒë∆∞·ª£c embedding n√†o cho '{folder_path}'.")
        return None
    return embeddings

def process_and_compare_audio(input_audio_path, encoder_model, base_embeddings):
    """X·ª≠ l√Ω file √¢m thanh v√† d·ª± ƒëo√°n ng∆∞·ªùi n√≥i b·∫±ng KNN."""
    if not os.path.exists(input_audio_path):
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file √¢m thanh ƒë·∫ßu v√†o: '{input_audio_path}'")
        return

    try:
        audio_test = AudioSegment.from_wav(input_audio_path)
        audio_test = audio_test.normalize()
        print(f"ƒê√£ chu·∫©n h√≥a √¢m l∆∞·ª£ng cho file ƒë·∫ßu v√†o '{input_audio_path}'")
    except Exception as e:
        print(f"‚ùå L·ªói ƒë·ªçc file √¢m thanh: {e}")
        return

    if audio_test.frame_rate != 16000:
        resampled_filename = get_unique_filename(RESAMPLED_PROCESSING_FILENAME)
        try:
            audio_test.export(resampled_filename, format="wav")
            audio_test = AudioSegment.from_wav(resampled_filename)
            audio_test = audio_test.normalize()
        except Exception as e:
            print(f"‚ùå L·ªói resampling: {e}")
            return
    else:
        resampled_filename = None

    min_duration_ms = 3000  # 3 seconds recommended
    if len(audio_test) < min_duration_ms:
        times = int(np.ceil(min_duration_ms / len(audio_test)))
        print(f"Nh√¢n b·∫£n √¢m thanh {times} l·∫ßn ƒë·ªÉ ƒë·∫°t ƒë·ªô d√†i t·ªëi thi·ªÉu.")
        extended_audio = extend_audio(audio_test, times)
        extended_audio = extended_audio.normalize()
    else:
        extended_audio = audio_test

    try:
        audio_embedding = get_embedding_from_audiosegment(extended_audio, encoder_model)
    except Exception as e:
        print(f"‚ùå Kh√¥ng th·ªÉ l·∫•y embedding: {e}")
        return

    # KNN prediction
    all_distances = []
    for speaker, embeddings in base_embeddings.items():
        for embedding, _ in embeddings:
            print(f"ƒêang t√≠nh kho·∫£ng c√°ch Cosine v·ªõi {speaker}...")
            print(f"  - S·ªë l∆∞·ª£ng embedding: {len(embeddings)}")
            print(f"  - K√≠ch th∆∞·ªõc embedding: {embedding}")
            distance = inference.compute_cosine_similarity(embedding, audio_embedding)
            all_distances.append((distance, speaker))
    
    # Sort by distance and select K nearest neighbors
    sorted_distances = sorted(all_distances, key=lambda x: x[0])
    knn_predictions = [speaker for _, speaker in sorted_distances[:K_NEAREST_NEIGHBOURS]]
    
    # Compute mean distances per speaker (for comparison with original method)
    mean_distances = {}
    for speaker in base_embeddings:
        distances = [inference.compute_cosine_similarity(emb, audio_embedding) for emb, _ in base_embeddings[speaker]]
        mean_distances[speaker] = np.mean(distances) if distances else float('inf')

    print("\n--- Kho·∫£ng c√°ch Cosine Trung B√¨nh ---")
    for speaker, distance in mean_distances.items():
        print(f"  - {speaker}: {distance:.4f}")

    # KNN prediction result
    predicted_speaker = Counter(knn_predictions).most_common(1)[0][0]
    print(f"\n--- K-Nearest Neighbors Prediction (K={K_NEAREST_NEIGHBOURS}) ---")
    print(f"Top {K_NEAREST_NEIGHBOURS} nearest neighbors: {knn_predictions}")
    print(f"\033[94mNg∆∞·ªùi ƒë∆∞·ª£c d·ª± ƒëo√°n (KNN): {predicted_speaker}\033[0m")

    # Original prediction for comparison
    min_distance = min(mean_distances.values())
    original_predicted_speaker = min(mean_distances, key=mean_distances.get)
    print(f"\nOriginal Prediction (Min Mean Distance): {original_predicted_speaker} (kho·∫£ng c√°ch: {min_distance:.4f})")

    if resampled_filename:
        try:
            os.remove(resampled_filename)
        except Exception:
            print(f"L·ªói x√≥a: {resampled_filename}")
    if USE_MICROPHONE_INPUT:
        try:
            os.remove(input_audio_path)
        except Exception:
            print(f"L·ªói x√≥a: {input_audio_path}")
    print("‚úÖ")

def record_audio_until_ctrl(filename=RAW_RECORDING_FILENAME):
    """Ghi √¢m t·ª´ micro cho ƒë·∫øn khi nh·∫•n Ctrl+C."""
    print("üé§ ƒêang ghi √¢m... Nh·∫•n Ctrl+C ƒë·ªÉ d·ª´ng.")
    audio = pyaudio.PyAudio()
    try:
        stream = audio.open(format=FORMAT,
                            channels=CHANNELS,
                            rate=RATE,
                            input=True,
                            frames_per_buffer=CHUNK)
        frames = []
        while True:
            try:
                if touch_sensor.is_pressed:
                    data = stream.read(CHUNK, exception_on_overflow=False)
                    frames.append(data)
            except KeyboardInterrupt:
                print("\n‚úÖ D·ª´ng ghi √¢m.")
                break

        full_filename = get_unique_filename(filename)
        with wave.open(full_filename, 'wb') as wf:
            wf.setnchannels(CHANNELS)
            wf.setsampwidth(audio.get_sample_size(FORMAT))
            wf.setframerate(RATE)
            wf.writeframes(b''.join(frames))
        print(f"ƒê√£ l∆∞u b·∫£n ghi √¢m t·∫°i: {full_filename}")
        return full_filename
    except Exception as e:
        print(f"‚ùå L·ªói khi ghi √¢m: {e}")
        return None
    finally:
        stream.stop_stream()
        stream.close()
        audio.terminate()

# --- Ch∆∞∆°ng tr√¨nh ch√≠nh ---
print("üîÑ ƒêang t·∫£i Speaker Encoder...")
encoder_path = "/home/pi/Desktop/09_06/IOT/saved_model/best-model-train-clean-360-hours-50000-epochs-specaug-80-mfcc-100-seq-len-full-inference-8-batch-3-stacks/saved_model_20240606215142.pt"
encoder = neural_net.get_speaker_encoder(encoder_path)
print("‚úÖ ƒê√£ t·∫£i model xong!")

print("\nüîÑ ƒêang t·∫£i m·∫´u gi·ªçng n√≥i c∆° s·ªü...")
base_embeddings = {
    "Tr√≠": prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples_folder/Tri", encoder),
    "Sum": prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples_folder/Sum", encoder),
    "Dat": prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples_folder/Dat", encoder),
    "Quan": prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples_folder/Quan", encoder),
    "Quang": prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples_folder/Quang", encoder),
    "Phat": prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples_folder/Phat", encoder),
    "Thanh": prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples_folder/Thanh", encoder)
}

if any(e is None for e in base_embeddings.values()):
    print("‚ùå L·ªói: Kh√¥ng th·ªÉ t·∫£i ƒë·∫ßy ƒë·ªß c√°c m·∫´u gi·ªçng n√≥i c∆° s·ªü.")
    exit()
print("‚úÖ ƒê√£ t·∫£i xong c√°c m·∫´u gi·ªçng n√≥i c∆° s·ªü!")

try:
    while True:
        print("\nüí° Nh·∫•n Enter ƒë·ªÉ nh·∫≠p WAV path ho·∫∑c ch·∫°m c·∫£m bi·∫øn (GPIO 22) ƒë·ªÉ ghi √¢m/x·ª≠ l√Ω file tƒ©nh.")
        print("Nh·∫≠p 'q' ƒë·ªÉ tho√°t.")
        
        touch_sensor.wait_for_press(timeout=0.1)
        if touch_sensor.is_pressed:
            print("\n--- C·∫£m bi·∫øn ƒë∆∞·ª£c ch·∫°m! ---")
            if USE_MICROPHONE_INPUT:
                input_audio_for_processing = record_audio_until_ctrl()
            else:
                input_audio_for_processing = STATIC_WAV_FILE_PATH
                print(f"ƒêang s·ª≠ d·ª•ng file: '{input_audio_for_processing}'")
            process_and_compare_audio(input_audio_for_processing, encoder, base_embeddings)
        else:
            user_input = input("Nh·∫≠p ƒë∆∞·ªùng d·∫´n WAV ho·∫∑c nh·∫•n Enter ƒë·ªÉ d√πng m·∫∑c ƒë·ªãnh, 'q' ƒë·ªÉ tho√°t: ")
            if user_input.lower() == 'q':
                break
            elif user_input.strip():
                input_audio_for_processing = user_input.strip()
                print(f"ƒêang x·ª≠ l√Ω file WAV: '{input_audio_for_processing}'")
            else:
                input_audio_for_processing = record_audio() if USE_MICROPHONE_INPUT else STATIC_WAV_FILE_PATH
                if not USE_MICROPHONE_INPUT:
                    print(f"ƒêang s·ª≠ d·ª•ng file m·∫∑c ƒë·ªãnh: '{input_audio_for_processing}'")
            
            process_and_compare_audio(input_audio_for_processing, encoder, base_embeddings)
        time.sleep(1)

except KeyboardInterrupt:
    print("\n‚õî D·ª´ng ch∆∞∆°ng tr√¨nh.")
except Exception as e:
    print(f"ƒê√£ x·∫£y ra l·ªói: {e}")
finally:
    touch_sensor.close()
    print("ƒê√£ ƒë√≥ng t√†i nguy√™n GPIO Zero.")