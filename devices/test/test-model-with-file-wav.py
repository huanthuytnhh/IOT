import os
import time
import wave
import tempfile
import numpy as np
import torch
import pyaudio
from pydub import AudioSegment
from gpiozero import Button
import speaker_recognition.inference as inference
import speaker_recognition.neural_net as neural_net

# --- Äá»‹nh nghÄ©a cÃ¡c háº±ng sá»‘ ---
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 44100  # Táº§n sá»‘ láº¥y máº«u máº·c Ä‘á»‹nh cá»§a micro, sáº½ resample vá» 16000Hz sau
CHUNK = 512
RECORD_SECONDS = 3  # Thá»i lÆ°á»£ng ghi Ã¢m
RAW_RECORDING_FILENAME = "test-new-model-mic/recorded_audio_raw.wav"
RESAMPLED_PROCESSING_FILENAME = "test-new-model-mic/recorded_audio_resampled_for_processing.wav"
N_TIMES_DUPLICATE = 1  # NhÃ¢n báº£n Ã¢m thanh Ä‘á»ƒ Ä‘á»§ thá»i lÆ°á»£ng náº¿u cáº§n

# --- Cáº¥u hÃ¬nh Ä‘áº§u vÃ o ---
USE_MICROPHONE_INPUT = False
# STATIC_WAV_FILE_PATH = "/home/pi/Desktop/PBL5/ai_model/data/user_C/user_C_50.wav"
# STATIC_WAV_FILE_PATH = "/home/pi/Desktop/PBL5/ai_model/data/user_E/user_E_17.wav"
STATIC_WAV_FILE_PATH = "/home/pi/Desktop/09_06/IOT/temp_recorded_audio/recording_resampled.wav"
# STATIC_WAV_FILE_PATH = "/home/pi/Desktop/PBL5/ai_model/data/user_D/user_D_50.wav"
# STATIC_WAV_FILE_PATH = "/home/pi/Desktop/PBL5/ai_model/data/user_A/user_A_20.wav"
# STATIC_WAV_FILE_PATH = "/home/pi/Desktop/PBL05_smart_home_with_voice_print/AI Module/speaker_recognition_using_lstm/Data Tiáº¿ng nÃ³i Ä‘á»ƒ Ä‘iá»u khiá»ƒn nhÃ /Äáº¡t/Dat_tat_den_garage.wav"
# STATIC_WAV_FILE_PATH = "/home/pi/Desktop/PBL05_smart_home_with_voice_print/AI Module/speaker_recognition_using_lstm/Data Tiáº¿ng nÃ³i Ä‘á»ƒ Ä‘iá»u khiá»ƒn nhÃ /PhÃ¡t/phat_bat-den-phong-bep.wav"
# STATIC_WAV_FILE_PATH = "/home/pi/Desktop/PBL05_smart_home_with_voice_print/AI Module/speaker_recognition_using_lstm/Data Tiáº¿ng nÃ³i Ä‘á»ƒ Ä‘iá»u khiá»ƒn nhÃ /TrÃ­/tri_tat_den_phong_ngu_con_cai.wav"
# Táº¡o thÆ° má»¥c náº¿u chÆ°a cÃ³
os.makedirs("test-new-model-mic", exist_ok=True)

# Cáº£m biáº¿n cháº¡m GPIO
touch_sensor = Button(22)

# --- HÃ m há»— trá»£ ---
def get_unique_filename(base_filename):
    """Táº¡o tÃªn file duy nháº¥t báº±ng cÃ¡ch thÃªm timestamp."""
    base, ext = os.path.splitext(base_filename)
    return f"{base}_{int(time.time())}{ext}"

def convert_sample_rate(input_filename, output_filename, target_sample_rate):
    """Chuyá»ƒn Ä‘á»•i táº§n sá»‘ láº¥y máº«u cá»§a file WAV."""
    # print(f"Äang chuyá»ƒn Ä‘á»•i sample rate cá»§a '{input_filename}' sang {target_sample_rate} Hz...")
    try:
        sound = AudioSegment.from_file(input_filename)
        sound = sound.set_frame_rate(target_sample_rate)
        sound.export(output_filename, format="wav")
        # print(f"ÄÃ£ lÆ°u file chuyá»ƒn Ä‘á»•i: '{output_filename}'")
    except Exception as e:
        print(f"Lá»—i khi chuyá»ƒn Ä‘á»•i sample rate: {e}")
        raise

def extend_audio(audio_segment, times=N_TIMES_DUPLICATE):
    """NhÃ¢n báº£n Ä‘oáº¡n Ã¢m thanh Ä‘á»ƒ kÃ©o dÃ i thá»i lÆ°á»£ng."""
    if times > 1:
        print(f"NhÃ¢n báº£n Ã¢m thanh {times} láº§n Ä‘á»ƒ kÃ©o dÃ i thá»i lÆ°á»£ng...")
    return audio_segment * times

def get_embedding_from_audiosegment(audio_segment, encoder_model):
    """Láº¥y embedding tá»« má»™t Ä‘á»‘i tÆ°á»£ng AudioSegment."""
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmpfile:
        audio_segment.export(tmpfile.name, format="wav")
        tmp_filename = tmpfile.name
    try:
        embedding = inference.get_embedding(tmp_filename, encoder_model)
    except Exception as e:
        print(f"Lá»—i khi tÃ­nh embedding cho '{tmp_filename}': {e}")
        raise
    finally:
        os.remove(tmp_filename)  # LuÃ´n xÃ³a file táº¡m
    return embedding

def record_audio(filename=RAW_RECORDING_FILENAME, record_seconds=RECORD_SECONDS):
    """Ghi Ã¢m tá»« micro vÃ  lÆ°u vÃ o file WAV."""
    print(f"ğŸ¤ Äang ghi Ã¢m trong {record_seconds} giÃ¢y...")
    audio = pyaudio.PyAudio()
    stream = audio.open(format=FORMAT,
                        channels=CHANNELS,
                        rate=RATE,
                        input=True,
                        frames_per_buffer=CHUNK)

    frames = []
    for _ in range(0, int(RATE / CHUNK * record_seconds)):
        data = stream.read(CHUNK, exception_on_overflow=False)
        frames.append(data)

    print("âœ… Káº¿t thÃºc ghi Ã¢m.")
    stream.stop_stream()
    stream.close()
    audio.terminate()

    full_filename = get_unique_filename(filename)
    wf = wave.open(full_filename, 'wb')
    wf.setnchannels(CHANNELS)
    wf.setsampwidth(audio.get_sample_size(FORMAT))
    wf.setframerate(RATE)  # Sá»­a lá»—i typo tá»« 'setframezrate' thÃ nh 'setframerate'
    wf.writeframes(b''.join(frames))
    wf.close()
    print(f"ÄÃ£ lÆ°u báº£n ghi Ã¢m thÃ´ táº¡i: '{full_filename}'")
    return full_filename

def process_and_compare_audio(input_audio_path, encoder_model, base_embeddings):
    """Xá»­ lÃ½ file Ã¢m thanh vÃ  so sÃ¡nh vá»›i cÃ¡c embedding cÆ¡ sá»Ÿ."""
    if not os.path.exists(input_audio_path):
        print(f"âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y file Ã¢m thanh Ä‘áº§u vÃ o: '{input_audio_path}'")
        return

    resampled_filename = get_unique_filename(RESAMPLED_PROCESSING_FILENAME)
    convert_sample_rate(input_audio_path, resampled_filename, 16000)
    
    audio_test = AudioSegment.from_wav(resampled_filename)
    extended_audio = extend_audio(audio_test)
    
    try:
        audio_embedding = get_embedding_from_audiosegment(extended_audio, encoder_model)
    except Exception as e:
        print(f"KhÃ´ng thá»ƒ láº¥y embedding tá»« file ghi Ã¢m: {e}")
        return

    users = ["TrÃ­","Sum", "Äáº¡t", "QuÃ¢n", "Quang", "PhÃ¡t", "Thanh"]
    distances = [
        inference.compute_cosine_similarity(base_embeddings["Tri"], audio_embedding),
        inference.compute_cosine_similarity(base_embeddings["Sum"], audio_embedding),
        inference.compute_cosine_similarity(base_embeddings["Dat"], audio_embedding),
        inference.compute_cosine_similarity(base_embeddings["Quan"], audio_embedding),
        inference.compute_cosine_similarity(base_embeddings["Quang"], audio_embedding),
        inference.compute_cosine_similarity(base_embeddings["Phat"], audio_embedding),
        inference.compute_cosine_similarity(base_embeddings["Thanh"], audio_embedding)
    ]

    print("\n--- Khoáº£ng cÃ¡ch Cosine ---")
    for user, distance in zip(users, distances):
        print(f"  - {user}: {distance:.4f}")
    
    min_distance = min(distances)
    min_distance_index = distances.index(min_distance)
    recognized_user = users[min_distance_index]
    print(f"\nâœ¨ NgÆ°á»i nÃ³i Ä‘Æ°á»£c nháº­n dáº¡ng lÃ : {recognized_user} (khoáº£ng cÃ¡ch: {min_distance:.4f})")

    os.remove(resampled_filename)
    if USE_MICROPHONE_INPUT:
        os.remove(input_audio_path)
    print("âœ… ÄÃ£ xá»­ lÃ½ xong vÃ  dá»n dáº¹p file táº¡m.")

def prepare_base_embedding(file_path, encoder_model):
    """TÃ­nh toÃ¡n embedding cÆ¡ sá»Ÿ cho má»™t ngÆ°á»i dÃ¹ng."""
    if not os.path.exists(file_path):
        print(f"âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y file máº«u '{file_path}'.")
        return None

    try:
        audio = AudioSegment.from_wav(file_path)
        print(f"Sample rate cá»§a file máº«u '{os.path.basename(file_path)}': {audio.frame_rate} Hz")
    except Exception as e:
        print(f"Lá»—i khi Ä‘á»c file máº«u '{file_path}': {e}")
        return None

    resampled_path = get_unique_filename("test-new-model-mic/temp_resampled_base.wav")
    convert_sample_rate(file_path, resampled_path, 16000)
    
    audio = AudioSegment.from_wav(resampled_path)
    extended = extend_audio(audio)
    
    embedding = None
    try:
        embedding = get_embedding_from_audiosegment(extended, encoder_model)
    except Exception as e:
        print(f"Lá»—i khi tÃ­nh embedding cho file máº«u '{resampled_path}': {e}")
    finally:
        os.remove(resampled_path)
    return embedding

# --- ChÆ°Æ¡ng trÃ¬nh chÃ­nh ---
print("ğŸ”„ Äang load model Speaker Encoder...")
encoder_path = "/home/pi/Desktop/09_06/IOT/saved_model/best-model-train-clean-360-hours-50000-epochs-specaug-80-mfcc-100-seq-len-full-inference-8-batch-3-stacks/saved_model_20240606215142.pt"
encoder = neural_net.get_speaker_encoder(encoder_path)
print("âœ… ÄÃ£ load model xong!")

print("\nğŸ”„ Äang load máº«u giá»ng nÃ³i cÆ¡ sá»Ÿ cá»§a ngÆ°á»i dÃ¹ng...")
base_embeddings = {}
base_embeddings["Tri"] = prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples/Tri-Merge_Audio.wav", encoder)
base_embeddings["Sum"] = prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples/Sum-Merge_Audio.wav", encoder)
base_embeddings["Dat"] = prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples/Dat-Merge_Audio.wav", encoder)
base_embeddings["Quan"] = prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples/Quan-Merge_Audio.wav", encoder)
base_embeddings["Quang"] = prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples/Quang-Merge_Audio.wav", encoder)
base_embeddings["Phat"] = prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples/Phat-Merge_Audio.wav", encoder)
base_embeddings["Thanh"] = prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples/Thanh-Merge_Audio.wav", encoder)

if any(e is None for e in base_embeddings.values()):
    print("âŒ Lá»—i: KhÃ´ng thá»ƒ load Ä‘áº§y Ä‘á»§ cÃ¡c máº«u giá»ng nÃ³i cÆ¡ sá»Ÿ. Vui lÃ²ng kiá»ƒm tra láº¡i Ä‘Æ°á»ng dáº«n file.")
    exit()
print("âœ… ÄÃ£ load xong cÃ¡c máº«u giá»ng nÃ³i cÆ¡ sá»Ÿ!")

try:
    if USE_MICROPHONE_INPUT:
        print("\nğŸ’¡ Cháº¡m vÃ o cáº£m biáº¿n (GPIO 22) Ä‘á»ƒ báº¯t Ä‘áº§u GHI Ã‚M vÃ  nháº­n dáº¡ng giá»ng nÃ³i...")
    else:
        print(f"\nğŸ’¡ Cháº¡m vÃ o cáº£m biáº¿n (GPIO 22) Ä‘á»ƒ xá»­ lÃ½ file: '{STATIC_WAV_FILE_PATH}' vÃ  nháº­n dáº¡ng giá»ng nÃ³i...")
    
    while True:
        touch_sensor.wait_for_press()
        print("\n--- Cáº£m biáº¿n Ä‘Æ°á»£c cháº¡m! ---")
        
        input_audio_for_processing = record_audio() if USE_MICROPHONE_INPUT else STATIC_WAV_FILE_PATH
        if not USE_MICROPHONE_INPUT:
            print(f"Äang sá»­ dá»¥ng file Ä‘Ã£ Ä‘á»‹nh nghÄ©a: '{input_audio_for_processing}'")
        
        process_and_compare_audio(input_audio_for_processing, encoder, base_embeddings)
        time.sleep(1)

except KeyboardInterrupt:
    print("\nâ›” Dá»«ng chÆ°Æ¡ng trÃ¬nh.")
except Exception as e:
    print(f"ÄÃ£ xáº£y ra lá»—i khÃ´ng mong muá»‘n: {e}")
finally:
    touch_sensor.close()
    print("ÄÃ£ Ä‘Ã³ng tÃ i nguyÃªn GPIO Zero.")