import os
import time
import wave
import tempfile
import numpy as np
import torch
import pyaudio
from pydub import AudioSegment
from gpiozero import Button
import speaker_recognition.inference as inference
import speaker_recognition.neural_net as neural_net

# --- ƒê·ªãnh nghƒ©a c√°c h·∫±ng s·ªë ---
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 44100  # T·∫ßn s·ªë l·∫•y m·∫´u m·∫∑c ƒë·ªãnh c·ªßa micro, s·∫Ω resample v·ªÅ 16000Hz sau
CHUNK = 512
RECORD_SECONDS = 3  # Th·ªùi l∆∞·ª£ng ghi √¢m
RAW_RECORDING_FILENAME = "test-new-model-mic/recorded_audio_raw.wav"
RESAMPLED_PROCESSING_FILENAME = "test-new-model-mic/recorded_audio_resampled_for_processing.wav"
N_TIMES_DUPLICATE = 1  # Nh√¢n b·∫£n √¢m thanh ƒë·ªÉ ƒë·ªß th·ªùi l∆∞·ª£ng n·∫øu c·∫ßn

# --- C·∫•u h√¨nh ƒë·∫ßu v√†o ---
USE_MICROPHONE_INPUT = False
# STATIC_WAV_FILE_PATH = "/home/pi/Desktop/PBL5/ai_model/data/user_C/user_C_50.wav"
# STATIC_WAV_FILE_PATH = "/home/pi/Desktop/PBL5/ai_model/data/user_E/user_E_17.wav"
STATIC_WAV_FILE_PATH = "/home/pi/Desktop/09_06/IOT/temp_recorded_audio/recording_resampled.wav"
# STATIC_WAV_FILE_PATH = "/home/pi/Desktop/PBL5/ai_model/data/user_D/user_D_50.wav"
# STATIC_WAV_FILE_PATH = "/home/pi/Desktop/PBL5/ai_model/data/user_A/user_A_20.wav"
# STATIC_WAV_FILE_PATH = "/home/pi/Desktop/PBL05_smart_home_with_voice_print/AI Module/speaker_recognition_using_lstm/Data Ti·∫øng n√≥i ƒë·ªÉ ƒëi·ªÅu khi·ªÉn nh√†/ƒê·∫°t/Dat_tat_den_garage.wav"
# STATIC_WAV_FILE_PATH = "/home/pi/Desktop/PBL05_smart_home_with_voice_print/AI Module/speaker_recognition_using_lstm/Data Ti·∫øng n√≥i ƒë·ªÉ ƒëi·ªÅu khi·ªÉn nh√†/Ph√°t/phat_bat-den-phong-bep.wav"
# STATIC_WAV_FILE_PATH = "/home/pi/Desktop/PBL05_smart_home_with_voice_print/AI Module/speaker_recognition_using_lstm/Data Ti·∫øng n√≥i ƒë·ªÉ ƒëi·ªÅu khi·ªÉn nh√†/Tr√≠/tri_tat_den_phong_ngu_con_cai.wav"
# T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
os.makedirs("test-new-model-mic", exist_ok=True)

# C·∫£m bi·∫øn ch·∫°m GPIO
touch_sensor = Button(22)

# --- H√†m h·ªó tr·ª£ ---
def get_unique_filename(base_filename):
    """T·∫°o t√™n file duy nh·∫•t b·∫±ng c√°ch th√™m timestamp."""
    base, ext = os.path.splitext(base_filename)
    return f"{base}_{int(time.time())}{ext}"

def convert_sample_rate(input_filename, output_filename, target_sample_rate):
    """Chuy·ªÉn ƒë·ªïi t·∫ßn s·ªë l·∫•y m·∫´u c·ªßa file WAV."""
    # print(f"ƒêang chuy·ªÉn ƒë·ªïi sample rate c·ªßa '{input_filename}' sang {target_sample_rate} Hz...")
    try:
        sound = AudioSegment.from_file(input_filename)
        sound = sound.set_frame_rate(target_sample_rate)
        sound.export(output_filename, format="wav")
        # print(f"ƒê√£ l∆∞u file chuy·ªÉn ƒë·ªïi: '{output_filename}'")
    except Exception as e:
        print(f"L·ªói khi chuy·ªÉn ƒë·ªïi sample rate: {e}")
        raise

def extend_audio(audio_segment, times=N_TIMES_DUPLICATE):
    """Nh√¢n b·∫£n ƒëo·∫°n √¢m thanh ƒë·ªÉ k√©o d√†i th·ªùi l∆∞·ª£ng."""
    if times > 1:
        print(f"Nh√¢n b·∫£n √¢m thanh {times} l·∫ßn ƒë·ªÉ k√©o d√†i th·ªùi l∆∞·ª£ng...")
    return audio_segment * times

def get_embedding_from_audiosegment(audio_segment, encoder_model):
    """L·∫•y embedding t·ª´ m·ªôt ƒë·ªëi t∆∞·ª£ng AudioSegment."""
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmpfile:
        audio_segment.export(tmpfile.name, format="wav")
        tmp_filename = tmpfile.name
    try:
        embedding = inference.get_embedding(tmp_filename, encoder_model)
    except Exception as e:
        print(f"L·ªói khi t√≠nh embedding cho '{tmp_filename}': {e}")
        raise
    finally:
        os.remove(tmp_filename)  # Lu√¥n x√≥a file t·∫°m
    return embedding

def record_audio(filename=RAW_RECORDING_FILENAME, record_seconds=RECORD_SECONDS):
    """Ghi √¢m t·ª´ micro v√† l∆∞u v√†o file WAV."""
    print(f"üé§ ƒêang ghi √¢m trong {record_seconds} gi√¢y...")
    audio = pyaudio.PyAudio()
    stream = audio.open(format=FORMAT,
                        channels=CHANNELS,
                        rate=RATE,
                        input=True,
                        frames_per_buffer=CHUNK)

    frames = []
    for _ in range(0, int(RATE / CHUNK * record_seconds)):
        data = stream.read(CHUNK, exception_on_overflow=False)
        frames.append(data)

    print("‚úÖ K·∫øt th√∫c ghi √¢m.")
    stream.stop_stream()
    stream.close()
    audio.terminate()

    full_filename = get_unique_filename(filename)
    wf = wave.open(full_filename, 'wb')
    wf.setnchannels(CHANNELS)
    wf.setsampwidth(audio.get_sample_size(FORMAT))
    wf.setframerate(RATE)  # S·ª≠a l·ªói typo t·ª´ 'setframezrate' th√†nh 'setframerate'
    wf.writeframes(b''.join(frames))
    wf.close()
    print(f"ƒê√£ l∆∞u b·∫£n ghi √¢m th√¥ t·∫°i: '{full_filename}'")
    return full_filename

def process_and_compare_audio(input_audio_path, encoder_model, base_embeddings):
    """X·ª≠ l√Ω file √¢m thanh v√† so s√°nh v·ªõi c√°c embedding c∆° s·ªü."""
    if not os.path.exists(input_audio_path):
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file √¢m thanh ƒë·∫ßu v√†o: '{input_audio_path}'")
        return

    resampled_filename = get_unique_filename(RESAMPLED_PROCESSING_FILENAME)
    convert_sample_rate(input_audio_path, resampled_filename, 16000)
    
    audio_test = AudioSegment.from_wav(resampled_filename)
    extended_audio = extend_audio(audio_test)
    
    try:
        audio_embedding = get_embedding_from_audiosegment(extended_audio, encoder_model)
    except Exception as e:
        print(f"Kh√¥ng th·ªÉ l·∫•y embedding t·ª´ file ghi √¢m: {e}")
        return

    users = ["Tr√≠","Sum", "ƒê·∫°t", "Qu√¢n", "Quang", "Ph√°t", "Thanh"]
    distances = [
        inference.compute_cosine_similarity(base_embeddings["Tri"], audio_embedding),
        inference.compute_cosine_similarity(base_embeddings["Sum"], audio_embedding),
        inference.compute_cosine_similarity(base_embeddings["Dat"], audio_embedding),
        inference.compute_cosine_similarity(base_embeddings["Quan"], audio_embedding),
        inference.compute_cosine_similarity(base_embeddings["Quang"], audio_embedding),
        inference.compute_cosine_similarity(base_embeddings["Phat"], audio_embedding),
        inference.compute_cosine_similarity(base_embeddings["Thanh"], audio_embedding)
    ]

    print("\n--- Kho·∫£ng c√°ch Cosine ---")
    for user, distance in zip(users, distances):
        print(f"  - {user}: {distance:.4f}")
    
    min_distance = min(distances)
    min_distance_index = distances.index(min_distance)
    recognized_user = users[min_distance_index]
    print(f"\n‚ú® Ng∆∞·ªùi n√≥i ƒë∆∞·ª£c nh·∫≠n d·∫°ng l√†: {recognized_user} (kho·∫£ng c√°ch: {min_distance:.4f})")

    os.remove(resampled_filename)
    if USE_MICROPHONE_INPUT:
        os.remove(input_audio_path)
    print("‚úÖ ƒê√£ x·ª≠ l√Ω xong v√† d·ªçn d·∫πp file t·∫°m.")

def prepare_base_embedding(file_path, encoder_model):
    """T√≠nh to√°n embedding c∆° s·ªü cho m·ªôt ng∆∞·ªùi d√πng."""
    if not os.path.exists(file_path):
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file m·∫´u '{file_path}'.")
        return None

    try:
        audio = AudioSegment.from_wav(file_path)
        print(f"Sample rate c·ªßa file m·∫´u '{os.path.basename(file_path)}': {audio.frame_rate} Hz")
    except Exception as e:
        print(f"L·ªói khi ƒë·ªçc file m·∫´u '{file_path}': {e}")
        return None

    resampled_path = get_unique_filename("test-new-model-mic/temp_resampled_base.wav")
    convert_sample_rate(file_path, resampled_path, 16000)
    
    audio = AudioSegment.from_wav(resampled_path)
    extended = extend_audio(audio)
    
    embedding = None
    try:
        embedding = get_embedding_from_audiosegment(extended, encoder_model)
    except Exception as e:
        print(f"L·ªói khi t√≠nh embedding cho file m·∫´u '{resampled_path}': {e}")
    finally:
        os.remove(resampled_path)
    return embedding

# --- Ch∆∞∆°ng tr√¨nh ch√≠nh ---
print("üîÑ ƒêang load model Speaker Encoder...")
encoder_path = "/home/pi/Desktop/09_06/IOT/saved_model/best-model-train-clean-360-hours-50000-epochs-specaug-80-mfcc-100-seq-len-full-inference-8-batch-3-stacks/saved_model_20240606215142.pt"
encoder = neural_net.get_speaker_encoder(encoder_path)
print("‚úÖ ƒê√£ load model xong!")

print("\nüîÑ ƒêang load m·∫´u gi·ªçng n√≥i c∆° s·ªü c·ªßa ng∆∞·ªùi d√πng...")
base_embeddings = {}
base_embeddings["Tri"] = prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples/Tri-Merge_Audio.wav", encoder)
base_embeddings["Sum"] = prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples/Sum-Merge_Audio.wav", encoder)
base_embeddings["Dat"] = prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples/Dat-Merge_Audio.wav", encoder)
base_embeddings["Quan"] = prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples/Quan-Merge_Audio.wav", encoder)
base_embeddings["Quang"] = prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples/Quang-Merge_Audio.wav", encoder)
base_embeddings["Phat"] = prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples/Phat-Merge_Audio.wav", encoder)
base_embeddings["Thanh"] = prepare_base_embedding("/home/pi/Desktop/09_06/IOT/audio_samples/Thanh-Merge_Audio.wav", encoder)

if any(e is None for e in base_embeddings.values()):
    print("‚ùå L·ªói: Kh√¥ng th·ªÉ load ƒë·∫ßy ƒë·ªß c√°c m·∫´u gi·ªçng n√≥i c∆° s·ªü. Vui l√≤ng ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n file.")
    exit()
print("‚úÖ ƒê√£ load xong c√°c m·∫´u gi·ªçng n√≥i c∆° s·ªü!")

try:
    if USE_MICROPHONE_INPUT:
        print("\nüí° Ch·∫°m v√†o c·∫£m bi·∫øn (GPIO 22) ƒë·ªÉ b·∫Øt ƒë·∫ßu GHI √ÇM v√† nh·∫≠n d·∫°ng gi·ªçng n√≥i...")
    else:
        print(f"\nüí° Ch·∫°m v√†o c·∫£m bi·∫øn (GPIO 22) ƒë·ªÉ x·ª≠ l√Ω file: '{STATIC_WAV_FILE_PATH}' v√† nh·∫≠n d·∫°ng gi·ªçng n√≥i...")
    
    while True:
        touch_sensor.wait_for_press()
        print("\n--- C·∫£m bi·∫øn ƒë∆∞·ª£c ch·∫°m! ---")
        
        input_audio_for_processing = record_audio() if USE_MICROPHONE_INPUT else STATIC_WAV_FILE_PATH
        if not USE_MICROPHONE_INPUT:
            print(f"ƒêang s·ª≠ d·ª•ng file ƒë√£ ƒë·ªãnh nghƒ©a: '{input_audio_for_processing}'")
        
        process_and_compare_audio(input_audio_for_processing, encoder, base_embeddings)
        time.sleep(1)

except KeyboardInterrupt:
    print("\n‚õî D·ª´ng ch∆∞∆°ng tr√¨nh.")
except Exception as e:
    print(f"ƒê√£ x·∫£y ra l·ªói kh√¥ng mong mu·ªën: {e}")
finally:
    touch_sensor.close()
    print("ƒê√£ ƒë√≥ng t√†i nguy√™n GPIO Zero.")